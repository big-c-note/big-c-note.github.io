<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta name="generator" content="Pelican" />
        <title>at home deeplearning - dev blog - misc</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">at home deeplearning - dev blog</a></h1>
                <nav><ul>
                    <li class="active"><a href="/category/misc.html">misc</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/exploring-low-rank-adaptation-with-tinygrad.html">Exploring Low Rank Adaptation with tinygrad</a></h1>
<footer class="post-info">
        <abbr class="published" title="2023-10-12T00:00:00-04:00">
                Published: Thu 12 October 2023
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/big-c-note.html">big-c-note</a>
        </address>
<p>In <a href="/category/misc.html">misc</a>.</p>

</footer><!-- /.post-info --><h2>Exploring Low Rank Adaptation with tinygrad</h2>
<p>In this post, I'll highlight <a href="https://github.com/big-c-note/tinygrad/blob/feature/add-lora-example/examples/lora_mnist.py">my project</a> that implements <a href="https://arxiv.org/abs/2106.09685">Low Rank Adaptation (LoRA)</a> using the <a href="https://tinygrad.org/">tinygrad framework</a> on a pre-trained model. The process involves simulating a pre-trained model with one epoch on the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, freezing the model parameters, identifying the worst classified digit, applying LoRA, and fine-tuning the model on training data filtered for that particular digit [1, 2]. The result shows improved performance on the identified digit without altering the frozen parameters.</p>
<p><img alt="LoRA Output Image" src="/images/lora1.png">
<img alt="LoRA Output Image" src="/images/lora2.png">
<strong>The LoRA parameters aren't technically considered "layers", just quicker to note it like that.</strong></p>
<h2>Introduction to MNIST, tinygrad and LoRA</h2>
<p>MNIST is a dataset consiting of images of digits 0-9. The images are often used to predict what integer it is a drawing of. It's known for being small withonly 50K examples.</p>
<p>tinygrad is a minimal neural network framework known for its simplicity and ease of integration with new accelerators for both inference and training.</p>
<p>LoRA is a method aimed at efficient fine-tuning of pre-trained models by reducing the number of parameters involved in fine-tuning, thus potentially speeding up the process and saving resources.</p>
<p>There's a great image of LoRA on the <a href="https://arxiv.org/pdf/2106.09685.pdf">first page of the original paper</a>. </p>
<h2>Benefits of tinygrad and LoRA</h2>
<ol>
<li><strong>Simplicity</strong>: tinygrad is simple, expressive, and pythonic which makes it easy to work with.</li>
<li><strong>Ease of Setup</strong>: <a href="https://big-c-note.github.io/setting-up-amd-raedon-vii-for-deeplearning-workloads-on-ubuntu-server-22043-featuring-tinygrad-with-opencl-accelerator.html">tinygrad easy to set up with AMD GPUs</a>, making it accessible for a broader range of developers, and making more competition for Nvidia.</li>
<li><strong>Efficiency</strong>: LoRA significantly reduces the number of parameters to be fine-tuned, making the process faster and less resource-intensive.</li>
</ol>
<h2>Implementing LoRA with tinygrad</h2>
<p>The provided code snippet demonstrates the process:</p>
<ol>
<li><strong>Simulating a Pre-trained Model</strong>: The model is simulated with one epoch on the MNIST dataset.</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
  <span class="o">...</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Simulating a pre-trained model, with one epoch..&quot;</span><span class="p">)</span>
  <span class="o">...</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">lossfn</span><span class="o">=</span><span class="n">lossfn</span><span class="p">,</span> <span class="n">BS</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
  <span class="o">...</span>
</code></pre></div>

<ol>
<li><strong>Freezing Model Parameters</strong>: After simulation, the model parameters are frozen to preserve the pre-trained weights.</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">loraize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="k">for</span> <span class="n">par</span> <span class="ow">in</span> <span class="n">original_params</span><span class="p">:</span>
    <span class="n">par</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="o">...</span>
</code></pre></div>

<ol>
<li>Identifying the Worst Classified Digit: The code identifies the digit 9 as the most poorly classified digit.</li>
</ol>
<div class="highlight"><pre><span></span><code>  <span class="o">...</span>
  <span class="n">worst_class</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">mislabeled_counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="n">mislabeled_counts</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worst class: </span><span class="si">{</span><span class="n">worst_class</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
  <span class="o">...</span>
</code></pre></div>

<ol>
<li>Applying LoRA: The model is loraized to prepare for fine-tuning.</li>
</ol>
<div class="highlight"><pre><span></span><code>  <span class="o">...</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Lora-izing the model..&quot;</span><span class="p">)</span>
  <span class="n">lora_model</span> <span class="o">=</span> <span class="n">loraize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
  <span class="nb">setattr</span><span class="p">(</span><span class="n">lora_model</span><span class="p">,</span> <span class="s2">&quot;forward&quot;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">lora_forward</span><span class="p">(</span><span class="n">lora_model</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span>
  <span class="o">...</span>
</code></pre></div>

<ol>
<li>Fine-tuning the Model: The model is fine-tuned on training data filtered for digit 9.</li>
</ol>
<div class="highlight"><pre><span></span><code>  <span class="o">...</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Fine-tuning the worst class, </span><span class="si">{</span><span class="n">worst_class</span><span class="si">}</span><span class="s2">..&quot;</span><span class="p">)</span>
  <span class="o">...</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span> <span class="o">=</span> <span class="n">filter_data_by_class</span><span class="p">(</span><span class="n">train_data</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">worst_class</span><span class="p">)</span>
  <span class="n">filtered_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_train</span><span class="p">)</span>
  <span class="o">...</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">lora_model</span><span class="p">,</span> <span class="n">filtered_data</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">filtered_data</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">lossfn</span><span class="o">=</span><span class="n">lossfn</span><span class="p">,</span> <span class="n">BS</span><span class="o">=</span><span class="n">BS</span><span class="p">)</span>
  <span class="o">...</span>
</code></pre></div>

<h2>Insights</h2>
<p>It's observed that with a certain extent of fine-tuning, the model significantly improves in classifying the digit worst class. However, over fine-tuning could lead to the model performing worse on other digits. This balance is crucial for achieving an overall efficient fine-tuning.</p>
<h2>References</h2>
<ul>
<li>Inspiration from [1] &amp; [2]</li>
<li>[1] https://www.youtube.com/watch?v=PXWYUTMt-AU</li>
<li>[2] https://colab.research.google.com/drive/13okPgkUeK8BrSMz5PXwQ_FXgUZgWxYLp</li>
</ul>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/setting-up-amd-raedon-vii-for-deeplearning-workloads-on-ubuntu-server-22043-featuring-tinygrad-with-opencl-accelerator.html" rel="bookmark"
                           title="Permalink to Setting up AMD Raedon VII for Deeplearning Workloads on Ubuntu Server 22.04.3 (Featuring Tinygrad with OpenCL Accelerator)">Setting up AMD Raedon VII for Deeplearning Workloads on Ubuntu Server 22.04.3 (Featuring Tinygrad with OpenCL Accelerator)</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2023-10-05T00:00:00-04:00">
                Published: Thu 05 October 2023
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/big-c-note.html">big-c-note</a>
        </address>
<p>In <a href="/category/misc.html">misc</a>.</p>

</footer><!-- /.post-info -->                <h2>Angel sucking at life, via stable diffusion via amd raedon vii</h2>
<p><img alt="Angel sucking at life" src="/images/angel-sucking-at-life.png"></p>
<h2>its tricky to set up an AMD gpu for use with pytorch</h2>
<p>I remember a few years ago my co-collaborator on a <a href="https://github.com/fedden/poker_ai">poker ai project</a> told me DO NOT buy AMD gpus because the tooling was "bleeding edge". Me …</p>
                <a class="readmore" href="/setting-up-amd-raedon-vii-for-deeplearning-workloads-on-ubuntu-server-22043-featuring-tinygrad-with-opencl-accelerator.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="https://getpelican.com/">Pelican</a></li>
                            <li><a href="https://www.python.org/">Python.org</a></li>
                            <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a href="https://www.python.org/">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>